{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vuthanhcdt/irobot_course/blob/main/humanoid_h1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XPBPyy92uwi"
      },
      "source": [
        "# Training the Humanoid Robot H1: A Reinforcement Learning Approach\n",
        "\n",
        "This notebook is introduced by the [Network Robotics System Laboratory (NRSL)](https://sites.google.com/site/yenchenliuncku) and is designed to train the humanoid robot H1 from Unitree [H1 Robot](https://www.unitree.com/h1). The source code is built upon [MuJoCo Playground](https://playground.mujoco.org/) and runs in a [MuJoCo](https://mujoco.org/) environment by Google DeepMind, which allows training policies in minutes on a single GPU with the [MuJoCo XLA (MJX) library](https://github.com/google-deepmind/mujoco/tree/main/mjx), a JAX-based implementation of MuJoCo, useful for reinforcement learning (RL) training workloads."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvyGCsgSCxHQ"
      },
      "source": [
        "# Install MuJoCo, MJX, and Brax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xqo7pyX-n72M"
      },
      "outputs": [],
      "source": [
        "#@title Install pre-requisites\n",
        "!pip install mujoco\n",
        "!pip install mujoco_mjx\n",
        "!pip install brax\n",
        "!pip install playground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbZxYDxzoz5R"
      },
      "outputs": [],
      "source": [
        "#@title Check if MuJoCo installation was successful\n",
        "\n",
        "from google.colab import files  # Use this if you are using Google Colab. If you are using a local machine, this may not work.\n",
        "\n",
        "import distutils.util\n",
        "import os\n",
        "import subprocess\n",
        "if subprocess.run('nvidia-smi').returncode:\n",
        "  raise RuntimeError(\n",
        "      'Cannot communicate with GPU. '\n",
        "      'Make sure you are using a GPU Colab runtime. '\n",
        "      'Go to the Runtime menu and select Choose runtime type.')\n",
        "\n",
        "# Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
        "# This is usually installed as part of an Nvidia driver package, but the Colab\n",
        "# kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
        "# (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
        "NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
        "if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
        "  with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
        "    f.write(\"\"\"{\n",
        "    \"file_format_version\" : \"1.0.0\",\n",
        "    \"ICD\" : {\n",
        "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
        "    }\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "# Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
        "print('Setting environment variable to use GPU rendering:')\n",
        "%env MUJOCO_GL=egl\n",
        "\n",
        "try:\n",
        "  print('Checking that the installation succeeded:')\n",
        "  import mujoco\n",
        "  mujoco.MjModel.from_xml_string('<mujoco/>')\n",
        "except Exception as e:\n",
        "  raise e from RuntimeError(\n",
        "      'Something went wrong during installation. Check the shell output above '\n",
        "      'for more information.\\n'\n",
        "      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
        "      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
        "\n",
        "print('Installation successful.')\n",
        "\n",
        "# Tell XLA to use Triton GEMM, this improves steps/sec by ~30% on some GPUs\n",
        "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
        "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
        "os.environ['XLA_FLAGS'] = xla_flags\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5f4w3Kq2X14"
      },
      "outputs": [],
      "source": [
        "#@title Import packages for plotting and creating graphics\n",
        "import time\n",
        "import itertools\n",
        "import numpy as np\n",
        "from typing import Callable, NamedTuple, Optional, Union, List\n",
        "\n",
        "# Graphics and plotting.\n",
        "print('Installing mediapy:')\n",
        "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
        "!pip install -q mediapy\n",
        "import mediapy as media\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# More legible printing from numpy.\n",
        "np.set_printoptions(precision=3, suppress=True, linewidth=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObF1UXrkb0Nd"
      },
      "outputs": [],
      "source": [
        "#@title Import MuJoCo, MJX, and Brax\n",
        "from datetime import datetime\n",
        "from etils import epath\n",
        "import functools\n",
        "from IPython.display import HTML\n",
        "from typing import Any, Dict, Sequence, Tuple, Union\n",
        "from typing import Any, Dict, Optional, Union\n",
        "import os\n",
        "from ml_collections import config_dict\n",
        "import imageio\n",
        "from datetime import datetime\n",
        "import datetime\n",
        "import jax\n",
        "from jax import numpy as jp\n",
        "import numpy as np\n",
        "from flax.training import orbax_utils\n",
        "from flax import struct\n",
        "from matplotlib import pyplot as plt\n",
        "import mediapy as media\n",
        "from orbax import checkpoint as ocp\n",
        "\n",
        "import mujoco\n",
        "from mujoco import mjx\n",
        "\n",
        "from brax import base\n",
        "from brax import envs\n",
        "from brax import math\n",
        "from brax.base import Base, Motion, Transform\n",
        "from brax.base import State as PipelineState\n",
        "from brax.envs.base import Env, PipelineEnv, State\n",
        "from brax.mjx.base import State as MjxState\n",
        "from brax.training.agents.ppo import train as ppo\n",
        "from brax.training.agents.ppo import networks as ppo_networks\n",
        "from brax.io import html, mjcf, model\n",
        "from typing import Any, Tuple\n",
        "from mujoco_playground._src import collision\n",
        "from mujoco_playground._src import gait\n",
        "from mujoco_playground._src import mjx_env\n",
        "from mujoco_playground._src.locomotion.h1 import base as h1_base\n",
        "from mujoco_playground._src.locomotion.h1 import h1_constants as consts\n",
        "from mujoco_playground.config import locomotion_params\n",
        "from mujoco_playground import wrapper\n",
        "from mujoco_playground import registry\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKU5XjA-2uwo"
      },
      "source": [
        "## Humanoid Unitree H1 Environment\n",
        "\n",
        "Let's define a Humanoid Unitree H1 environment. To define the environment, modify the reward function, and support other robots, refer to the [MuJoCo Playground](https://playground.mujoco.org/) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V131knXs2uwp"
      },
      "outputs": [],
      "source": [
        "_PHASES = np.array([\n",
        "    [0, np.pi],  # walk\n",
        "    [0, 0],  # jump\n",
        "])\n",
        "\n",
        "\n",
        "def default_config() -> config_dict.ConfigDict:\n",
        "  return config_dict.create(\n",
        "      ctrl_dt=0.02,\n",
        "      sim_dt=0.004,\n",
        "      episode_length=1000,\n",
        "      early_termination=True,\n",
        "      action_repeat=1,\n",
        "      action_scale=0.3,\n",
        "      history_len=1,\n",
        "      obs_noise=config_dict.create(\n",
        "          level=0.6,\n",
        "          scales=config_dict.create(\n",
        "              joint_pos=0.01,\n",
        "              joint_vel=1.5,\n",
        "              gyro=0.2,\n",
        "              gravity=0.05,\n",
        "          ),\n",
        "      ),\n",
        "      reward_config=config_dict.create(\n",
        "          scales=config_dict.create(\n",
        "              # Rewards.\n",
        "              feet_phase=5.0,\n",
        "              tracking_lin_vel=3.5,\n",
        "              tracking_ang_vel=0.75,\n",
        "              feet_air_time=0.0,\n",
        "              # Costs.\n",
        "              ang_vel_xy=-0.0,\n",
        "              lin_vel_z=-0.0,\n",
        "              pose=-2.5,\n",
        "              foot_slip=-0.0,\n",
        "              action_rate=-0.01,\n",
        "          ),\n",
        "          tracking_sigma=0.5,\n",
        "      ),\n",
        "      command_config=config_dict.create(\n",
        "          lin_vel_x=[-1.5, 1.5],\n",
        "          lin_vel_y=[-0.5, 0.5],\n",
        "          ang_vel_yaw=[-1.0, 1.0],\n",
        "          lin_vel_threshold=0.1,\n",
        "          ang_vel_threshold=0.1,\n",
        "      ),\n",
        "      gait_frequency=[0.5, 2.0],\n",
        "      gaits=[\"walk\"],\n",
        "      foot_height=[0.08, 0.4],\n",
        "  )\n",
        "\n",
        "\n",
        "class JoystickGaitTracking(h1_base.H1Env):\n",
        "  \"\"\"\n",
        "  A class for tracking joystick-controlled gait in a simulated environment.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      config: config_dict.ConfigDict = default_config(),\n",
        "      config_overrides: Optional[Dict[str, Union[str, int, list[Any]]]] = None,\n",
        "  ):\n",
        "\n",
        "\n",
        "    super().__init__(\n",
        "        # xml_path = epath.Path('unitree_h1/scene_mjx_feetonly.xml').as_posix(),\n",
        "        xml_path=consts.FEET_ONLY_XML.as_posix(),\n",
        "        config=config,\n",
        "        config_overrides=config_overrides,\n",
        "    )\n",
        "    self._config = config\n",
        "    self._post_init()\n",
        "\n",
        "  def _post_init(self) -> None:\n",
        "    self._init_q = jp.array(self._mj_model.keyframe(\"home\").qpos)\n",
        "    self._default_pose = self._mj_model.keyframe(\"home\").qpos[7:]\n",
        "    self._lowers = self._mj_model.actuator_ctrlrange[:, 0]\n",
        "    self._uppers = self._mj_model.actuator_ctrlrange[:, 1]\n",
        "\n",
        "    self._hx_idxs = jp.array([\n",
        "        0, 1, 4,  # left leg\n",
        "        5, 6, 9,  # right leg\n",
        "        10,  # torso\n",
        "        11, 12, 13, 14,  # left arm\n",
        "        15, 16, 17, 18,  # right arm\n",
        "    ])  # fmt: skip\n",
        "    self._weights = jp.array([\n",
        "        5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 2.0, 1.0, 1.0, 1.0, 1.0,\n",
        "        1.0, 1.0, 1.0, 1.0,\n",
        "    ])  # fmt: skip\n",
        "\n",
        "    self._hx_default_pose = self._default_pose[self._hx_idxs]\n",
        "\n",
        "    self._feet_site_id = np.array(\n",
        "        [self._mj_model.site(name).id for name in consts.FEET_SITES]\n",
        "    )\n",
        "    self._floor_geom_id = self._mj_model.geom(\"floor\").id\n",
        "    self._left_feet_geom_id = np.array(\n",
        "        [self._mj_model.geom(name).id for name in consts.LEFT_FEET_GEOMS]\n",
        "    )\n",
        "    self._right_feet_geom_id = np.array(\n",
        "        [self._mj_model.geom(name).id for name in consts.RIGHT_FEET_GEOMS]\n",
        "    )\n",
        "    foot_linvel_sensor_adr = []\n",
        "    for site in consts.FEET_SITES:\n",
        "      sensor_id = self._mj_model.sensor(f\"{site}_global_linvel\").id\n",
        "      sensor_adr = self._mj_model.sensor_adr[sensor_id]\n",
        "      sensor_dim = self._mj_model.sensor_dim[sensor_id]\n",
        "      foot_linvel_sensor_adr.append(\n",
        "          list(range(sensor_adr, sensor_adr + sensor_dim))\n",
        "      )\n",
        "    self._foot_linvel_sensor_adr = jp.array(foot_linvel_sensor_adr)\n",
        "\n",
        "  def reset(self, rng: jax.Array) -> mjx_env.State:\n",
        "    rng, noise_rng, gait_freq_rng, gait_rng, foot_height_rng, cmd_rng = (  # pylint: disable=redefined-outer-name\n",
        "        jax.random.split(rng, 6)\n",
        "    )\n",
        "\n",
        "    data = mjx_env.init(\n",
        "        self.mjx_model, qpos=self._init_q, qvel=jp.zeros(self.mjx_model.nv)\n",
        "    )\n",
        "\n",
        "    # Initialize history buffers.\n",
        "    qpos_error_history = jp.zeros(self._config.history_len * 19)\n",
        "    qvel_history = jp.zeros(self._config.history_len * 19)\n",
        "\n",
        "    # Sample gait parameters.\n",
        "    gait_freq = jax.random.uniform(\n",
        "        gait_freq_rng,\n",
        "        minval=self._config.gait_frequency[0],\n",
        "        maxval=self._config.gait_frequency[1],\n",
        "    )\n",
        "    phase_dt = 2 * jp.pi * self.dt * gait_freq\n",
        "    gait = jax.random.randint(  # pylint: disable=redefined-outer-name\n",
        "        gait_rng, minval=0, maxval=len(self._config.gaits), shape=()\n",
        "    )\n",
        "    phase = jp.array(_PHASES)[gait]\n",
        "    foot_height = jax.random.uniform(\n",
        "        foot_height_rng,\n",
        "        minval=self._config.foot_height[0],\n",
        "        maxval=self._config.foot_height[1],\n",
        "    )\n",
        "\n",
        "    info = {\n",
        "        \"command\": self.sample_command(cmd_rng),\n",
        "        \"rng\": rng,\n",
        "        \"last_act\": jp.zeros(self.mjx_model.nu),\n",
        "        \"last_last_act\": jp.zeros(self.mjx_model.nu),\n",
        "        \"step\": 0,\n",
        "        \"motor_targets\": jp.zeros(self.mjx_model.nu),\n",
        "        \"qpos_error_history\": qpos_error_history,\n",
        "        \"qvel_history\": qvel_history,\n",
        "        \"swing_peak\": jp.zeros(2),\n",
        "        \"feet_air_time\": jp.zeros(2),\n",
        "        \"last_contact\": jp.zeros(2, dtype=bool),\n",
        "        \"lin_vel\": jp.zeros(3),\n",
        "        \"ang_vel\": jp.zeros(3),\n",
        "        \"gait_freq\": gait_freq,\n",
        "        \"gait\": gait,\n",
        "        \"phase\": phase,\n",
        "        \"phase_dt\": phase_dt,\n",
        "        \"foot_height\": foot_height,\n",
        "        \"torques\": data.actuator_force,\n",
        "        \"base_height\": data.qpos[2]\n",
        "    }\n",
        "\n",
        "    metrics = {}\n",
        "    for k in self._config.reward_config.scales.keys():\n",
        "      metrics[f\"reward/{k}\"] = jp.zeros(())\n",
        "\n",
        "    left_feet_contact = jp.array([\n",
        "        collision.geoms_colliding(data, geom_id, self._floor_geom_id)\n",
        "        for geom_id in self._left_feet_geom_id\n",
        "    ])\n",
        "    right_feet_contact = jp.array([\n",
        "        collision.geoms_colliding(data, geom_id, self._floor_geom_id)\n",
        "        for geom_id in self._right_feet_geom_id\n",
        "    ])\n",
        "    contact = jp.hstack([jp.any(left_feet_contact), jp.any(right_feet_contact)])\n",
        "    obs = self._get_obs(data, info, noise_rng, contact)\n",
        "    reward, done = jp.zeros(2)\n",
        "    return mjx_env.State(data, obs, reward, done, metrics, info)\n",
        "\n",
        "  def step(self, state: mjx_env.State, action: jax.Array) -> mjx_env.State:\n",
        "    rng, cmd_rng, noise_rng = jax.random.split(state.info[\"rng\"], 3)\n",
        "\n",
        "    motor_targets = state.data.ctrl + action * self._config.action_scale\n",
        "    motor_targets = jp.clip(motor_targets, self._lowers, self._uppers)\n",
        "    data = mjx_env.step(\n",
        "        self.mjx_model, state.data, motor_targets, self.n_substeps\n",
        "    )\n",
        "    state.info[\"motor_targets\"] = motor_targets\n",
        "\n",
        "    left_feet_contact = jp.array([\n",
        "        collision.geoms_colliding(data, geom_id, self._floor_geom_id)\n",
        "        for geom_id in self._left_feet_geom_id\n",
        "    ])\n",
        "    right_feet_contact = jp.array([\n",
        "        collision.geoms_colliding(data, geom_id, self._floor_geom_id)\n",
        "        for geom_id in self._right_feet_geom_id\n",
        "    ])\n",
        "    contact = jp.hstack([jp.any(left_feet_contact), jp.any(right_feet_contact)])\n",
        "    contact_filt = contact | state.info[\"last_contact\"]\n",
        "    first_contact = (state.info[\"feet_air_time\"] > 0.0) * contact_filt\n",
        "    state.info[\"feet_air_time\"] += self.dt\n",
        "    p_f = data.site_xpos[self._feet_site_id]\n",
        "    p_fz = p_f[..., -1]\n",
        "    state.info[\"swing_peak\"] = jp.maximum(state.info[\"swing_peak\"], p_fz)\n",
        "\n",
        "    obs = self._get_obs(data, state.info, noise_rng, contact)\n",
        "    done = self._get_termination(data)\n",
        "\n",
        "\n",
        "    # ====================================== Reward Function ============================================================\n",
        "    # Compute positive and negative reward components\n",
        "    pos, neg = self._get_reward(\n",
        "        data, action, state.info, state.metrics, done, first_contact, contact\n",
        "    )\n",
        "\n",
        "    # Apply individual scaling factors (weights) to each reward component\n",
        "    pos = {k: v * self._config.reward_config.scales[k] for k, v in pos.items()}\n",
        "    neg = {k: v * self._config.reward_config.scales[k] for k, v in neg.items()}\n",
        "\n",
        "    # Merge all components into a single dictionary\n",
        "    rewards = pos | neg  # Using `|` to combine the two dictionaries (Python 3.9+)\n",
        "\n",
        "    # ================= Optional alternative reward formulation =================\n",
        "    # Sum of positive rewards\n",
        "    # r_pos = sum(pos.values())\n",
        "    # Exponential scaling of negative costs (to smoothly penalize large costs)\n",
        "    # r_neg = jp.exp(0.2 * sum(neg.values()))\n",
        "    # Final reward = positive reward × scaled penalty × time step\n",
        "    # reward = r_pos * r_neg * self.dt\n",
        "\n",
        "    # ================= Simplified reward computation (currently used) =================\n",
        "    # Final reward: sum all scaled components and clip to be non-negative\n",
        "    reward = jp.clip(sum(rewards.values()) * self.dt, 0.0)\n",
        "    # ===============================================================================================================\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    state.info[\"last_last_act\"] = state.info[\"last_act\"]\n",
        "    state.info[\"last_act\"] = action\n",
        "    state.info[\"step\"] += 1\n",
        "    phase_tp1 = state.info[\"phase\"] + state.info[\"phase_dt\"]\n",
        "    state.info[\"phase\"] = jp.fmod(phase_tp1 + jp.pi, 2 * jp.pi) - jp.pi\n",
        "    state.info[\"rng\"] = rng\n",
        "    state.info[\"feet_air_time\"] *= ~contact\n",
        "    state.info[\"last_contact\"] = contact\n",
        "    state.info[\"swing_peak\"] *= ~contact\n",
        "\n",
        "    state.info[\"command\"] = jp.where(\n",
        "        state.info[\"step\"] > 500,\n",
        "        self.sample_command(cmd_rng),\n",
        "        state.info[\"command\"],\n",
        "    )\n",
        "    state.info[\"step\"] = jp.where(\n",
        "        done | (state.info[\"step\"] > 500),\n",
        "        0,\n",
        "        state.info[\"step\"],\n",
        "    )\n",
        "\n",
        "    state.info[\"torques\"] = data.actuator_force\n",
        "    state.info[\"lin_vel\"] = self.get_local_linvel(data)\n",
        "    state.info[\"ang_vel\"] = self.get_gyro(data)\n",
        "    state.info[\"base_height\"] = data.qpos[2]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for k, v in rewards.items():\n",
        "      state.metrics[f\"reward/{k}\"] = v\n",
        "\n",
        "    done = done.astype(reward.dtype)\n",
        "    state = state.replace(data=data, obs=obs, reward=reward, done=done)\n",
        "    return state\n",
        "\n",
        "  def _get_termination(self, data: mjx.Data) -> jax.Array:\n",
        "    # Terminates if joint limits are exceeded or the robot falls.\n",
        "    joint_angles = data.qpos[7:]\n",
        "    joint_limit_exceed = jp.any(joint_angles < self._lowers)\n",
        "    joint_limit_exceed |= jp.any(joint_angles > self._uppers)\n",
        "    fall_termination = self.get_gravity(data)[-1] < 0.85\n",
        "    return jp.where(\n",
        "        self._config.early_termination,\n",
        "        joint_limit_exceed | fall_termination,\n",
        "        joint_limit_exceed,\n",
        "    )\n",
        "\n",
        "  def _get_obs(\n",
        "      self,\n",
        "      data: mjx.Data,\n",
        "      info: dict[str, Any],\n",
        "      rng: jax.Array,\n",
        "      contact: jax.Array,\n",
        "  ) -> jax.Array:\n",
        "    obs = jp.concatenate([\n",
        "        self.get_gyro(data),  # 3\n",
        "        self.get_gravity(data),  # 3\n",
        "        data.qpos[7:] - self._default_pose,  # 19\n",
        "        data.qvel[6:],  # 19\n",
        "        info[\"last_act\"],  # 19\n",
        "        info[\"command\"],  # 3\n",
        "    ])  # 66\n",
        "\n",
        "    # Add noise.\n",
        "    noise_vec = jp.zeros_like(obs)\n",
        "    noise_vec = noise_vec.at[:3].set(\n",
        "        self._config.obs_noise.level * self._config.obs_noise.scales.gyro\n",
        "    )\n",
        "    noise_vec = noise_vec.at[3:6].set(\n",
        "        self._config.obs_noise.level * self._config.obs_noise.scales.gravity\n",
        "    )\n",
        "    noise_vec = noise_vec.at[6:25].set(\n",
        "        self._config.obs_noise.level * self._config.obs_noise.scales.joint_pos\n",
        "    )\n",
        "    noise_vec = noise_vec.at[25:44].set(\n",
        "        self._config.obs_noise.level * self._config.obs_noise.scales.joint_vel\n",
        "    )\n",
        "    obs = obs + (2 * jax.random.uniform(rng, shape=obs.shape) - 1) * noise_vec\n",
        "\n",
        "    # Update history.\n",
        "    qvel_history = jp.roll(info[\"qvel_history\"], 19).at[:19].set(data.qvel[6:])\n",
        "    qpos_error_history = (\n",
        "        jp.roll(info[\"qpos_error_history\"], 19)\n",
        "        .at[:19]\n",
        "        .set(data.qpos[7:] - info[\"motor_targets\"])\n",
        "    )\n",
        "    info[\"qvel_history\"] = qvel_history\n",
        "    info[\"qpos_error_history\"] = qpos_error_history\n",
        "\n",
        "    cos = jp.cos(info[\"phase\"])\n",
        "    sin = jp.sin(info[\"phase\"])\n",
        "    phase = jp.concatenate([cos, sin])\n",
        "\n",
        "    # Concatenate final observation.\n",
        "    obs = jp.hstack(\n",
        "        [\n",
        "            obs,\n",
        "            qvel_history,\n",
        "            qpos_error_history,\n",
        "            contact,\n",
        "            phase,\n",
        "            info[\"gait_freq\"],\n",
        "            info[\"gait\"],\n",
        "            info[\"foot_height\"],\n",
        "        ],\n",
        "    )\n",
        "    return obs\n",
        "\n",
        "  # ====================================== Reward Function ============================================================\n",
        "  # STUDENT NOTE:\n",
        "  # This is the core reward function that governs the robot's behavior during training.\n",
        "  # It computes and returns two sets of values:\n",
        "  #     - `pos`: A dictionary of positive rewards that encourage desired behaviors.\n",
        "  #     - `neg`: A dictionary of negative costs or penalties that discourage undesirable behaviors.\n",
        "  #\n",
        "  # You are encouraged to MODIFY or EXTEND this function to explore different robot behaviors.\n",
        "  # Consider the following possibilities:\n",
        "  #     ✔ Add a reward term to encourage **energy efficiency** (e.g., reducing torque usage).\n",
        "  #     ✔ Implement rewards for **specific movements** like jumping, running, or maintaining balance during dynamic actions.\n",
        "  #     ✔ Introduce penalties for **joint torque saturation** to avoid robot damage or inefficiencies.\n",
        "  #     ✔ Experiment with tuning the **balance between tracking precision** (linear and angular velocities) and **energy efficiency**.\n",
        "  # 🚨 IMPORTANT:\n",
        "  # Be mindful that any changes you make will directly affect the robot's training behavior and outcomes.\n",
        "\n",
        "  def _get_reward(\n",
        "      self,\n",
        "      data: mjx.Data,\n",
        "      action: jax.Array,\n",
        "      info: dict[str, Any],\n",
        "      metrics: dict[str, Any],\n",
        "      done: jax.Array,\n",
        "      first_contact: jax.Array,\n",
        "      contact: jax.Array,\n",
        "  ) -> tuple[dict[str, jax.Array], dict[str, jax.Array]]:\n",
        "    \"\"\"\n",
        "    Computes the reward components for the current simulation step.\n",
        "\n",
        "    This function evaluates both positive rewards (incentives for desired behavior)\n",
        "    and negative rewards (penalties for undesired or unsafe behavior). The total reward\n",
        "    is typically computed as a weighted sum of these components elsewhere in the code.\n",
        "\n",
        "    Args:\n",
        "        data (mjx.Data): The current state of the simulation.\n",
        "        action (jax.Array): The action applied at this timestep.\n",
        "        info (dict): A dictionary containing additional state and command information.\n",
        "        metrics (dict): Logging or tracking metrics (not used here).\n",
        "        done (jax.Array): Indicates whether the episode has terminated (not used).\n",
        "        first_contact (jax.Array): Contact indicators for detecting initial foot-ground impact.\n",
        "        contact (jax.Array): Contact states of the robot's feet with the environment.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of dictionaries:\n",
        "            - `pos`: Dictionary of positive reward components.\n",
        "            - `neg`: Dictionary of negative cost/penalty components.\n",
        "    \"\"\"\n",
        "\n",
        "    del done, metrics  # These arguments are unused in this reward computation.\n",
        "\n",
        "    # ---------------------- Positive Rewards ----------------------\n",
        "    pos = {\n",
        "        # Reward for accurately tracking the commanded linear velocity\n",
        "        \"tracking_lin_vel\": self._reward_tracking_lin_vel(\n",
        "            info[\"command\"], self.get_local_linvel(data)\n",
        "        ),\n",
        "        # Reward for matching the commanded angular velocity (yaw rate)\n",
        "        \"tracking_ang_vel\": self._reward_tracking_ang_vel(\n",
        "            info[\"command\"], self.get_gyro(data)\n",
        "        ),\n",
        "        # Encourages phase-consistent foot movements\n",
        "        \"feet_phase\": self._reward_feet_phase(\n",
        "            data, info[\"phase\"], info[\"foot_height\"]\n",
        "        ),\n",
        "        # Reward based on desired air-time of each foot (promotes dynamic walking)\n",
        "        \"feet_air_time\": self._reward_feet_air_time(\n",
        "            info[\"feet_air_time\"], first_contact, info[\"command\"]\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    # ---------------------- Negative Costs / Penalties ----------------------\n",
        "    neg = {\n",
        "        # Penalizes angular velocities in the XY plane (lateral instability)\n",
        "        \"ang_vel_xy\": self._cost_ang_vel_xy(self.get_global_angvel(data)),\n",
        "        # Penalizes unwanted vertical linear movement\n",
        "        \"lin_vel_z\": self._cost_lin_vel_z(\n",
        "            self.get_global_linvel(data), info[\"gait\"]\n",
        "        ),\n",
        "        # Penalizes deviation from a natural pose\n",
        "        \"pose\": self._cost_pose(data.qpos[7:]),\n",
        "        # Penalizes slipping feet on contact (lack of traction)\n",
        "        \"foot_slip\": self._cost_foot_slip(data, contact),\n",
        "        # Penalizes abrupt or jerky action changes\n",
        "        \"action_rate\": self._cost_action_rate(\n",
        "            info[\"last_act\"], info[\"last_last_act\"], action\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    return pos, neg\n",
        "\n",
        "  def _reward_feet_phase(\n",
        "      self, data: mjx.Data, phase: jax.Array, foot_height: jax.Array\n",
        "  ) -> jax.Array:\n",
        "    # Reward for tracking the desired foot height.\n",
        "    foot_pos = data.site_xpos[self._feet_site_id]\n",
        "    foot_z = foot_pos[..., -1]\n",
        "    rz = gait.get_rz(phase, swing_height=foot_height)\n",
        "    error = jp.sum(jp.square(foot_z - rz))\n",
        "    return jp.exp(-error / 0.01)\n",
        "\n",
        "  def _reward_tracking_lin_vel(\n",
        "      self,\n",
        "      commands: jax.Array,\n",
        "      local_vel: jax.Array,\n",
        "  ) -> jax.Array:\n",
        "    # Tracking of linear velocity commands (xy axes).\n",
        "    lin_vel_error = jp.sum(jp.square(commands[:2] - local_vel[:2]))\n",
        "    reward = jp.exp(-lin_vel_error / self._config.reward_config.tracking_sigma)\n",
        "    return reward\n",
        "\n",
        "  def _reward_tracking_ang_vel(\n",
        "      self,\n",
        "      commands: jax.Array,\n",
        "      ang_vel: jax.Array,\n",
        "  ) -> jax.Array:\n",
        "    # Tracking of angular velocity commands (yaw).\n",
        "    ang_vel_error = jp.square(commands[2] - ang_vel[2])\n",
        "    return jp.exp(-ang_vel_error / self._config.reward_config.tracking_sigma)\n",
        "\n",
        "  def _reward_feet_air_time(\n",
        "      self, air_time: jax.Array, first_contact: jax.Array, commands: jax.Array\n",
        "  ) -> jax.Array:\n",
        "    # Reward air time.\n",
        "    cmd_norm = jp.linalg.norm(commands[:2])\n",
        "    rew_air_time = jp.sum((air_time - 0.1) * first_contact)\n",
        "    rew_air_time *= cmd_norm > 0.05  # No reward for zero commands.\n",
        "    return rew_air_time\n",
        "\n",
        "  def _cost_pose(self, joint_angles: jax.Array) -> jax.Array:\n",
        "    # Penalize deviation from the default pose for certain joints.\n",
        "    current = joint_angles[self._hx_idxs]\n",
        "    return jp.sum(jp.square(current - self._hx_default_pose) * self._weights)\n",
        "\n",
        "  def _cost_lin_vel_z(self, global_linvel, gait: jax.Array) -> jax.Array:  # pylint: disable=redefined-outer-name\n",
        "    # Penalize z axis base linear velocity unless pronk or bound.\n",
        "    cost = jp.square(global_linvel[2])\n",
        "    return cost * (gait > 0)\n",
        "\n",
        "  def _cost_ang_vel_xy(self, global_angvel) -> jax.Array:\n",
        "    # Penalize xy axes base angular velocity.\n",
        "    return jp.sum(jp.square(global_angvel[:2]))\n",
        "\n",
        "  def _cost_foot_slip(self, data: mjx.Data, contact: jax.Array) -> jax.Array:\n",
        "    feet_vel = data.sensordata[self._foot_linvel_sensor_adr]\n",
        "    vel_xy = feet_vel[..., :2]\n",
        "    vel_xy_norm_sq = jp.sum(jp.square(vel_xy), axis=-1)\n",
        "    return jp.sum(vel_xy_norm_sq * contact)\n",
        "\n",
        "  def _cost_action_rate(\n",
        "      self, act: jax.Array, last_act: jax.Array, last_last_act: jax.Array\n",
        "  ) -> jax.Array:\n",
        "    # Penalize first and second derivative of actions.\n",
        "    c1 = jp.sum(jp.square(act - last_act))\n",
        "    c2 = jp.sum(jp.square(act - 2 * last_act + last_last_act))\n",
        "    return c1 + c2\n",
        "  #======================================reward function============================================================\n",
        "\n",
        "\n",
        "\n",
        "  def sample_command(self, rng: jax.Array) -> jax.Array:\n",
        "    \"\"\"Samples a random command with a 10% chance of being zero.\"\"\"\n",
        "    _, rng1, rng2, rng3 = jax.random.split(rng, 4)\n",
        "    cmd_config = self._config.command_config\n",
        "    lin_vel_x = jax.random.uniform(\n",
        "        rng1, minval=cmd_config.lin_vel_x[0], maxval=cmd_config.lin_vel_x[1]\n",
        "    )\n",
        "    lin_vel_y = jax.random.uniform(\n",
        "        rng2, minval=cmd_config.lin_vel_y[0], maxval=cmd_config.lin_vel_y[1]\n",
        "    )\n",
        "    ang_vel_yaw = jax.random.uniform(\n",
        "        rng3,\n",
        "        minval=cmd_config.ang_vel_yaw[0],\n",
        "        maxval=cmd_config.ang_vel_yaw[1],\n",
        "    )\n",
        "    lin_vel_x = jp.where(\n",
        "        jp.abs(lin_vel_x) < cmd_config.lin_vel_threshold, 0, lin_vel_x\n",
        "    )\n",
        "    lin_vel_y = jp.where(\n",
        "        jp.abs(lin_vel_y) < cmd_config.lin_vel_threshold, 0, lin_vel_y\n",
        "    )\n",
        "    ang_vel_yaw = jp.where(\n",
        "        jp.abs(ang_vel_yaw) < cmd_config.ang_vel_threshold, 0, ang_vel_yaw\n",
        "    )\n",
        "    cmd = jp.hstack([lin_vel_x, lin_vel_y, ang_vel_yaw])\n",
        "    return cmd\n",
        "\n",
        "envs.register_environment('humanoid_h1', JoystickGaitTracking)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi7_z8-S2uwp"
      },
      "source": [
        "## Visualize a Rollout\n",
        "\n",
        "Let's instantiate the environment and visualize a short rollout without any policy. Since the robot is not controlled by a policy, it will fall over.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjqScISU2uwq"
      },
      "outputs": [],
      "source": [
        "# instantiate the environment\n",
        "env_name = 'humanoid_h1'\n",
        "env = envs.get_environment(env_name)\n",
        "\n",
        "# define the jit reset/step functions\n",
        "jit_reset = jax.jit(env.reset)\n",
        "jit_step = jax.jit(env.step)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Render the robot without a policy, and the robot will fall down. Please download the video and attach it to your assignment."
      ],
      "metadata": {
        "id": "3vMOr57qb7Ky"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHc3-fo92uwq"
      },
      "outputs": [],
      "source": [
        "# initialize the state\n",
        "state = jit_reset(jax.random.PRNGKey(0))\n",
        "rollout = [state]\n",
        "\n",
        "# grab a trajectory\n",
        "for i in range(100):\n",
        "  ctrl = -0.1 * jp.ones(env.mjx_model.nu)\n",
        "  state = jit_step(state, ctrl)\n",
        "  rollout.append(state)\n",
        "\n",
        "media.show_video(env.render(rollout, camera='track'), fps=1.0 / env.dt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7K_Q3Mk2uwq"
      },
      "source": [
        "## Train Humanoid Policy\n",
        "\n",
        "Let's now train a policy using PPO to make the Humanoid run forward. Training takes about 9 minutes on an RTX A6000 GPU, and approximately 30 minutes on Google Colab using an NVIDIA Tesla T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssxoEpht2uwq"
      },
      "outputs": [],
      "source": [
        "x_data = []\n",
        "y_data = []\n",
        "ydataerr = []\n",
        "times = [datetime.now()]\n",
        "randomizer = None # No domain randomization\n",
        "\n",
        "make_networks_factory = functools.partial(\n",
        "    ppo_networks.make_ppo_networks,\n",
        "        policy_hidden_layer_sizes=(128, 128, 128, 128),\n",
        "        value_hidden_layer_sizes=(256, 256, 256, 256))\n",
        "\n",
        "train_fn = functools.partial(\n",
        "    ppo.train, num_timesteps=100_000_000, num_evals=10, reward_scaling=1.0,\n",
        "    episode_length=1000, normalize_observations=True, action_repeat=1,\n",
        "    unroll_length=20, num_minibatches=32, num_updates_per_batch=4,\n",
        "    discounting=0.97, learning_rate=0.0003, entropy_cost=0.01, num_envs=8192,\n",
        "    batch_size=256, max_grad_norm=1.0, network_factory=make_networks_factory, randomization_fn=randomizer, seed=0)\n",
        "\n",
        "\n",
        "def progress(num_steps, metrics):\n",
        "  times.append(datetime.now())\n",
        "  x_data.append(num_steps)\n",
        "  y_data.append(metrics['eval/episode_reward'])\n",
        "  ydataerr.append(metrics['eval/episode_reward_std'])\n",
        "  plt.xlim([0, train_fn.keywords['num_timesteps'] * 1.25])\n",
        "  plt.xlabel('environment steps')\n",
        "  plt.ylabel('reward per episode')\n",
        "  plt.title(f'y={y_data[-1]:.3f}')\n",
        "  plt.errorbar(x_data, y_data, yerr=ydataerr, color=\"blue\")\n",
        "  display(plt.gcf())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAj9Tvv42uwr"
      },
      "source": [
        "During training, if the reward doesn't increase or fluctuates constantly, and ends lower than at the start, your reward function might not be effective. In this case, revise the reward components and try again to ensure stable progress and better performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEZUQw-o2uwr"
      },
      "outputs": [],
      "source": [
        "# Run training and retrieve the inference function and model parameters\n",
        "make_inference_fn, params, _ = train_fn(environment=env, progress_fn=progress, wrap_env_fn=wrapper.wrap_for_brax_training)\n",
        "\n",
        "# Measure time spent in JIT compilation and training\n",
        "time_to_jit = times[1] - times[0]\n",
        "time_to_train = times[-1] - times[1]\n",
        "\n",
        "print(f'Time to JIT compile: {time_to_jit}')\n",
        "print(f'Time to train: {time_to_train}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saved the training reward plot as `training_progress_<timestamp>.png` in the `plots` directory."
      ],
      "metadata": {
        "id": "xY2JvpzgYCG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "# Plot training reward with error bars\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.errorbar(x_data, y_data, yerr=ydataerr, color=\"blue\")\n",
        "plt.xlabel('Environment steps')\n",
        "plt.ylabel('Reward per episode')\n",
        "plt.title(\n",
        "    f'Final reward: {y_data[-1]:.3f}\\n'\n",
        "    f'Time to JIT: {time_to_jit}, Time to train: {time_to_train}'\n",
        ")\n",
        "plt.grid(True)\n",
        "\n",
        "# Ensure the 'plots' directory exists\n",
        "os.makedirs(\"plots\", exist_ok=True)\n",
        "\n",
        "# Generate timestamp for unique file naming\n",
        "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M')\n",
        "\n",
        "# Define path to save the plot\n",
        "plot_path = os.path.join(\"plots\", f\"training_progress_{timestamp}.png\")\n",
        "\n",
        "# Save the plot to file\n",
        "plt.savefig(plot_path, bbox_inches='tight')\n",
        "print(f\"[✔] Plot saved to: {plot_path}\")"
      ],
      "metadata": {
        "id": "eGGnRGL2YGnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLDR0YYE2uwr"
      },
      "source": [
        "<!-- ## Save and Load Policy -->\n",
        "\n",
        "We can save and load the policy using the brax model API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RxZIt6n2uwr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime\n",
        "\n",
        "# Define model save path (within the current directory)\n",
        "model_dir = './models'\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "# Get the current timestep or timestamp (you can customize this as needed)\n",
        "timestep = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "# Save the model parameters directly into the model_dir with timestep in the filename\n",
        "model_path = os.path.join(model_dir, f'humanoid_policy_h1_{timestep}')\n",
        "model.save_params(model_path, params)\n",
        "\n",
        "print(f\"Model saved to: {model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJBaUuQJ2uwr"
      },
      "outputs": [],
      "source": [
        "#@title Load Model and Define Inference Function\n",
        "params = model.load_params(model_path)\n",
        "\n",
        "inference_fn = make_inference_fn(params)\n",
        "jit_inference_fn = jax.jit(inference_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brX-uvfS2uwr"
      },
      "source": [
        "## Visualize Policy\n",
        "\n",
        "To evaluate the learned policy, four videos were rendered from different camera angles: `track`, `side`, `back`, and `front`. These perspectives help illustrate the robot's behavior from multiple viewpoints. Please download the videos to your computer and include them in your assignment submission. Additionally, the CSV files containing recorded data, the trained model parameters, and a plot of the reward function have been saved in their respective directories: `csvs`, `models`, and `plots`. You are also required to download these files and attach them to your assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yqAICEr2uwr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import datetime\n",
        "import imageio\n",
        "\n",
        "# Get the environment to evaluate the policy\n",
        "eval_env = envs.get_environment(env_name)\n",
        "\n",
        "# JIT compile the reset and step functions for faster execution\n",
        "jit_reset = jax.jit(eval_env.reset)\n",
        "jit_step = jax.jit(eval_env.step)\n",
        "\n",
        "# Set the episode length for each trial\n",
        "episode_length = 1000\n",
        "\n",
        "# Define velocities for the command (linear and angular velocities)\n",
        "x_vel = 1.0\n",
        "y_vel = 0.0\n",
        "yaw_vel = 0.0\n",
        "command = jp.array([x_vel, y_vel, yaw_vel])\n",
        "\n",
        "# Set the phase dynamics for the environment\n",
        "phase_dt = 2 * jp.pi * eval_env.dt * 1.5\n",
        "phase = jp.array([0, jp.pi])\n",
        "\n",
        "# Initialize the random number generator\n",
        "rng = jax.random.PRNGKey(1)\n",
        "\n",
        "# Initialize list to store episode data (torques, velocity, etc.)\n",
        "rollout = []\n",
        "modify_scene_fns = []\n",
        "\n",
        "# Create lists to store data for each DataFrame\n",
        "torque_data = []\n",
        "base_height_data = []\n",
        "lin_vel_body_data = []\n",
        "ang_vel_body_data = []\n",
        "\n",
        "# Get the current timestamp for filenames to make them unique\n",
        "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M')\n",
        "\n",
        "# Define the output directory for the CSV files\n",
        "output_dir = \"csvs\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Run one episode (can change range to run multiple episodes)\n",
        "for j in range(1):\n",
        "    print(f\"Episode {j}\")\n",
        "    # Reset the environment at the start of each episode\n",
        "    state = jit_reset(rng)\n",
        "    state.info[\"phase_dt\"] = phase_dt\n",
        "    state.info[\"phase\"] = phase\n",
        "\n",
        "    for i in range(episode_length):\n",
        "        # Split RNG and compute the control for the current state\n",
        "        act_rng, rng = jax.random.split(rng)\n",
        "        ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
        "        # Take a step in the environment with the computed control\n",
        "        state = jit_step(state, ctrl)\n",
        "\n",
        "        # If the episode is done, stop the loop\n",
        "        if state.done:\n",
        "            break\n",
        "\n",
        "        # Skip step 0 and do not save data\n",
        "        if i == 0:\n",
        "            continue\n",
        "\n",
        "        # Update the state info with the control command\n",
        "        state.info[\"command\"] = command\n",
        "\n",
        "        # Extract data from the state for torque, base height, velocities\n",
        "        torque = state.info[\"torques\"]\n",
        "        base_height = state.info[\"base_height\"]\n",
        "        lin_vel_body = state.info[\"lin_vel\"]\n",
        "        ang_vel_body = state.info[\"ang_vel\"]\n",
        "\n",
        "        # Create dynamic columns for torque values\n",
        "        torque_dict = {\"step\": i}\n",
        "        for j, t in enumerate(torque):\n",
        "            torque_dict[f\"torque_{j}\"] = t  # Add dynamic column for each torque component\n",
        "\n",
        "        # Create dynamic columns for linear body velocity with specific names\n",
        "        lin_vel_dict = {\"step\": i}\n",
        "        lin_vel_dict[\"lin_vel_x\"] = lin_vel_body[0]  # Set x component as lin_vel_x\n",
        "        lin_vel_dict[\"lin_vel_y\"] = lin_vel_body[1]  # Set y component as lin_vel_y\n",
        "        lin_vel_dict[\"lin_vel_z\"] = lin_vel_body[2]  # Set z component as lin_vel_z\n",
        "\n",
        "        # Create dynamic columns for angular body velocity with specific names\n",
        "        ang_vel_dict = {\"step\": i}\n",
        "        ang_vel_dict[\"ang_vel_x\"] = ang_vel_body[0]  # Set x component as ang_vel_x\n",
        "        ang_vel_dict[\"ang_vel_y\"] = ang_vel_body[1]  # Set y component as ang_vel_y\n",
        "        ang_vel_dict[\"ang_vel_z\"] = ang_vel_body[2]  # Set z component as ang_vel_z\n",
        "\n",
        "        # Append data to respective lists\n",
        "        torque_data.append(torque_dict)\n",
        "        base_height_data.append({\"step\": i, \"base_height\": base_height})\n",
        "        lin_vel_body_data.append(lin_vel_dict)\n",
        "        ang_vel_body_data.append(ang_vel_dict)\n",
        "\n",
        "        # Append the current state to the rollout list\n",
        "        rollout.append(state)\n",
        "\n",
        "# Convert the collected data into Pandas DataFrames\n",
        "torque_df = pd.DataFrame(torque_data)\n",
        "base_height_df = pd.DataFrame(base_height_data)\n",
        "lin_vel_body_df = pd.DataFrame(lin_vel_body_data)\n",
        "ang_vel_body_df = pd.DataFrame(ang_vel_body_data)\n",
        "\n",
        "# Save the data to CSV files in the output directory\n",
        "csv_filenames = {\n",
        "    \"torque\": os.path.join(output_dir, f\"torque_{x_vel}_{y_vel}_{yaw_vel}_{timestamp}.csv\"),\n",
        "    \"base_height\": os.path.join(output_dir, f\"base_height_{x_vel}_{y_vel}_{yaw_vel}_{timestamp}.csv\"),\n",
        "    \"lin_vel_body\": os.path.join(output_dir, f\"lin_vel_body_{x_vel}_{y_vel}_{yaw_vel}_{timestamp}.csv\"),\n",
        "    \"ang_vel_body\": os.path.join(output_dir, f\"ang_vel_body_{x_vel}_{y_vel}_{yaw_vel}_{timestamp}.csv\")\n",
        "}\n",
        "\n",
        "# Save DataFrames to CSV files\n",
        "torque_df.to_csv(csv_filenames[\"torque\"], index=False)\n",
        "base_height_df.to_csv(csv_filenames[\"base_height\"], index=False)\n",
        "lin_vel_body_df.to_csv(csv_filenames[\"lin_vel_body\"], index=False)\n",
        "ang_vel_body_df.to_csv(csv_filenames[\"ang_vel_body\"], index=False)\n",
        "\n",
        "print(f\"CSV files saved for torque, base_height, lin_vel_body, and ang_vel_body in folder: {output_dir}.\")\n",
        "\n",
        "# Render the robot's motion for video generation (this part remains unchanged)\n",
        "render_every = 1\n",
        "fps = 1.0 / eval_env.dt / render_every\n",
        "print(f\"FPS: {fps}\")\n",
        "traj = rollout[::render_every]\n",
        "\n",
        "# Define scene options for visualization\n",
        "scene_option = mujoco.MjvOption()\n",
        "scene_option.geomgroup[2] = True  # Show specific geometry group\n",
        "scene_option.geomgroup[3] = False  # Hide other geometry group\n",
        "scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True  # Display contact points\n",
        "scene_option.flags[mujoco.mjtVisFlag.mjVIS_TRANSPARENT] = False  # Make objects opaque\n",
        "scene_option.flags[mujoco.mjtVisFlag.mjVIS_PERTFORCE] = False  # Disable visualization of perturbation forces\n",
        "\n",
        "# List of camera views to render\n",
        "camera_views = [\"track\", \"side\", \"back\", \"front\"]\n",
        "rendered_frames = {}\n",
        "\n",
        "# Render and save frames for each camera view\n",
        "for view in camera_views:\n",
        "    frames = eval_env.render(\n",
        "        traj,\n",
        "        camera=view,\n",
        "        scene_option=scene_option,\n",
        "        width=640,\n",
        "        height=480,\n",
        "    )\n",
        "    rendered_frames[view] = frames\n",
        "    # Show video for the current view (optional)\n",
        "    media.show_video(frames, fps=fps, loop=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTEL7UcO2uwr"
      },
      "source": [
        "## Verify Policy\n",
        "\n",
        "Please implement your policy evaluation logic below. Your evaluation should cover the following key metrics:\n",
        "\n",
        "- **Total torque usage**  \n",
        "- **Command tracking performance**  \n",
        "- **Robot's base height over time**\n",
        "\n",
        "All evaluation results — including CSV files and plots — should be saved inside the `student_results` directory.\n",
        "\n",
        "**Final Submission Reminder:**  \n",
        "Once your evaluation is complete, compress all relevant materials — including your `Jupyter notebook`, `videos`, `CSV files`, `trained models`, `plots`, and the entire `student_results` folder — into a single archive. Upload this archive to [Moodle](https://moodle.ncku.edu.tw/) to receive your grade.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avPGwa9f2uwr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define the directory path\n",
        "results_dir = \"student_results\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Directory ready: {results_dir}\")\n",
        "\n",
        "# === Your policy evaluation code goes below ===\n",
        "# Write code here to evaluate your trained policy.\n",
        "# Save your results (e.g., torque, base height, command tracking) into CSV files,\n",
        "# and generate plots. All outputs should be stored in the 'student_results' directory.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuClass": "premium",
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "isaaclab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}